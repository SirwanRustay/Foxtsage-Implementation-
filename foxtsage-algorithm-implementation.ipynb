{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sirwanabdolwahedaula/foxtsage-algorithm-implementation?scriptVersionId=218625878\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### **Foxtsage Algorithm Implementation**\n\nWelcome to the Kaggle notebook demonstrating the implementation of the **Foxtsage** algorithm. This notebook provides both the code and detailed insights into how the hybrid **Foxtsage optimizer** works, as well as a comparison of its performance against the **Adam optimizer**.\n\n---\n\n### **Foxtsage: A Hybrid Algorithm for Dynamic Learning Rate Optimization**\n\nThe proposed algorithm, **Foxtsage**, is derived from the **Hybrid FOX-TSA** optimization technique. Foxtsage is specifically designed to dynamically update the learning rate of the **Stochastic Gradient Descent (SGD)** optimizer. By combining the strengths of the **FOX (Fox Optimization)** and **TSA (Tree-Seed Algorithm)**, Foxtsage aims to optimize the training process for various machine learning models, ensuring both efficient convergence and enhanced performance.\n\n---\n\n### **Foxtsage: Combining FOX and TSA for Dynamic Learning Rate Optimization**\n\n1. **Hybrid FOX-TSA: The Core of Foxtsage**  \n   - **FOX Algorithm**: Provides global exploration of the search space, minimizing the likelihood of getting stuck in local optima.  \n   - **TSA Algorithm**: Focuses on local exploitation, refining potential solutions around promising learning rates.  \n   - By combining these strategies, the Hybrid FOX-TSA strikes a balance between exploration and exploitation, offering an effective search for the optimal learning rate.\n\n2. **Dynamic Learning Rate Optimization with Foxtsage**  \n   - **Foxtsage** utilizes the Hybrid FOX-TSA approach to iteratively optimize the learning rate (\\( \\eta \\)) of the SGD optimizer.  \n   - The process involves:  \n     - Initializing a population of learning rates within a specified range (e.g., [0.0001, 0.01]).  \n     - Applying FOX-inspired updates for global exploration of potential learning rates.  \n     - Refining these candidates using TSA’s local exploitation capabilities.  \n   - The best-performing learning rate is selected based on a fitness metric, such as the model’s training loss, to guide the optimization process.\n\n3. **Integration into SGD-Based Models**  \n   - The dynamically optimized learning rate is used with SGD during the training of various machine learning models, including **Logistic Regression**, **Multi-Layer Perceptron (MLP)**, and **Convolutional Neural Networks (CNN)**.  \n   - This dynamic learning rate adjustment helps the SGD optimizer adapt to changing training conditions, improving convergence speed and model performance.\n\n---\n\n### **Key Features and Benefits of Foxtsage**\n- **Dynamic Learning Rate Adjustment**: Optimizes the learning rate throughout the training process, improving SGD's efficiency and performance.  \n- **Balanced Optimization**: Combines the global search power of FOX with the local refinement abilities of TSA to enhance learning rate tuning.  \n- **Versatile Application**: Can be applied to a variety of machine learning models, including classification tasks with datasets like MNIST and CIFAR-10, as well as engineering design problems.  \n- **Enhanced Model Performance**: Improves metrics like accuracy, precision, recall, and F1-score, demonstrating Foxtsage’s robustness across multiple tasks.\n\n---\n\n### **Foxtsage Algorithm Implementation**\n\nIn this notebook, each code cell is designed to implement the **Foxtsage algorithm** on a different dataset and machine learning model. The purpose is to demonstrate the versatility and robustness of the Foxtsage optimizer across a variety of tasks.\n\n- **Logistic Regression**: This model is used in one section of the notebook, applied to the **MNIST dataset** for digit classification. The **Foxtsage optimizer** adjusts the learning rate of SGD during the training process, aiming to improve the model's accuracy and convergence.\n  \n- **Multi-Layer Perceptron (MLP)**: In another section, the **Foxtsage algorithm** is tested on the **CIFAR-10 dataset** using an MLP model. The goal is to optimize the learning rate for better performance on more complex, high-dimensional data.\n\n- **Convolutional Neural Networks (CNN)**: The **CNN model** is used with both **CIFAR-10** and **MNIST** datasets to demonstrate how the **Foxtsage optimizer** can be applied to more complex models in deep learning. By dynamically adjusting the learning rate, **Foxtsage** aims to achieve faster convergence and improved accuracy in image classification tasks.\n\nEach dataset and model implementation is supported by the **Foxtsage optimizer**, ensuring that the learning rate is optimized to provide the best results.\n\n---\n\n### **Citations**\n1. Aula, S. A., & Rashid, T. A. (2024). Foxtsage vs. Adam: Revolution or Evolution in Optimization? *arXiv*. [https://doi.org/10.48550/arXiv.2412.17855](https://doi.org/10.48550/arXiv.2412.17855)  \n2. Aula, S. A., & Rashid, T. A. (2024). FOX-TSA Hybrid Algorithm: Advancing for Superior Predictive Accuracy in Tourism-Driven Multi-Layer Perceptron Models. *Systems and Soft Computing, Volume 6*. [https://doi.org/10.1016/j.sasc.2024.200178](https://doi.org/10.1016/j.sasc.2024.200178)  \n3. Aula, S. A., & Rashid, T. A. (2025). FOX-TSA: Navigating Complex Search Spaces and Superior Performance in Benchmark and Real-World Optimization Problems. *Ain Shams Engineering Journal, Volume 16, Issue 1*. [https://doi.org/10.1016/j.asej.2024.103185](https://doi.org/10.1016/j.asej.2024.103185)\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"### **Implementing Foxtsage for Logistic Regression Model on MNIST Dataset**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load MNIST Dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\ntest_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n\n# Logistic Regression Model\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten input\n        return self.linear(x)\n\n# Evaluation Function\ndef evaluate_model(model, loader, criterion):\n    model.eval()\n    total_loss = 0\n    preds, labels = [], []\n    with torch.no_grad():\n        for images, labels_batch in loader:\n            images, labels_batch = images.to(device), labels_batch.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels_batch)\n            total_loss += loss.item()\n            _, batch_preds = torch.max(outputs, 1)\n            preds.extend(batch_preds.cpu().numpy())\n            labels.extend(labels_batch.cpu().numpy())\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average=\"weighted\")\n    recall = recall_score(labels, preds, average=\"weighted\")\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return total_loss / len(loader), accuracy, precision, recall, f1\n\n# Save Metrics to Excel\ndef save_metrics_to_excel(metrics, filename, sheet_name):\n    pd.DataFrame(metrics).to_excel(filename, sheet_name=sheet_name, index=False)\n\n# Hybrid FOX-TSA Implementation\ndef run_hybrid_fox_tsa(iterations=50, population_size=30):\n    input_dim = 28 * 28\n    output_dim = 10\n    model = LogisticRegression(input_dim, output_dim).to(device)\n\n    learning_rate_bounds = (0.0001, 0.01)\n    population = np.random.uniform(learning_rate_bounds[0], learning_rate_bounds[1], population_size)\n    best_lr = population[0]\n    best_loss = float(\"inf\")\n\n    criterion = nn.CrossEntropyLoss()\n    metrics = {\"Iteration\": [], \"Loss\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for iteration in range(iterations):\n        start_time = time.time()\n        for i in range(population_size):\n            r = np.random.rand()\n            if r < 0.5:\n                population[i] = best_lr * (1 + np.random.randn() * 0.1)\n            else:\n                population[i] = best_lr / (1 + np.random.randn() * 0.1)\n\n            # Clip learning rate\n            population[i] = np.clip(population[i], learning_rate_bounds[0], learning_rate_bounds[1])\n            optimizer = optim.SGD(model.parameters(), lr=population[i])\n\n            # Train for one epoch\n            model.train()\n            epoch_loss = 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_lr = population[i]\n\n        # Evaluate the model\n        eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = evaluate_model(model, test_loader, criterion)\n        elapsed_time = time.time() - start_time\n        metrics[\"Iteration\"].append(iteration + 1)\n        metrics[\"Loss\"].append(eval_loss)\n        metrics[\"Accuracy\"].append(eval_accuracy)\n        metrics[\"Precision\"].append(eval_precision)\n        metrics[\"Recall\"].append(eval_recall)\n        metrics[\"F1-Score\"].append(eval_f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Hybrid FOX-TSA Iteration {iteration + 1}/{iterations}: Loss = {eval_loss:.4f}, Accuracy = {eval_accuracy:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Adam Implementation\ndef run_adam(iterations=50):\n    input_dim = 28 * 28\n    output_dim = 10\n    model = LogisticRegression(input_dim, output_dim).to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics = {\"Iteration\": [], \"Loss\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for epoch in range(iterations):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        # Evaluate the model\n        eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = evaluate_model(model, test_loader, criterion)\n        elapsed_time = time.time() - start_time\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Loss\"].append(eval_loss)\n        metrics[\"Accuracy\"].append(eval_accuracy)\n        metrics[\"Precision\"].append(eval_precision)\n        metrics[\"Recall\"].append(eval_recall)\n        metrics[\"F1-Score\"].append(eval_f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Adam Epoch {epoch + 1}/{iterations}: Loss = {eval_loss:.4f}, Accuracy = {eval_accuracy:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Run Experiments\nprint(\"\\nRunning Hybrid FOX-TSA...\")\nfox_tsa_metrics = run_hybrid_fox_tsa()\nsave_metrics_to_excel(fox_tsa_metrics, \"MNIST_Logistic_Hybrid_FOX_TSA_Metrics.xlsx\", \"Hybrid_FOX_TSA\")\n\nprint(\"\\nRunning Adam...\")\nadam_metrics = run_adam()\nsave_metrics_to_excel(adam_metrics, \"MNIST_Logistic_Adam_Metrics.xlsx\", \"Adam\")\n\n# Plot Results\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Loss\"], label=\"Hybrid FOX-TSA Loss\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Loss\"], label=\"Adam Loss\")\nplt.title(\"MNIST Logistic Regression - Loss Comparison\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Loss\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"MNIST_Logistic_Regression_Loss_Comparison.png\", dpi=300)\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Accuracy\"], label=\"Hybrid FOX-TSA Accuracy\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Accuracy\"], label=\"Adam Accuracy\")\nplt.title(\"MNIST Logistic Regression - Accuracy Comparison\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"MNIST_Logistic_Regression_Accuracy_Comparison.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:32:33.389095Z","iopub.execute_input":"2024-12-03T18:32:33.389469Z","iopub.status.idle":"2024-12-03T20:50:35.114035Z","shell.execute_reply.started":"2024-12-03T18:32:33.389434Z","shell.execute_reply":"2024-12-03T20:50:35.113088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implementing Logistic Regression with Foxtsage on IMDB Dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tensorflow.keras.datasets import imdb\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom openpyxl import Workbook\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load IMDB Dataset and Preprocess\ndef load_imdb_data():\n    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n\n    word_index = imdb.get_word_index()\n    reverse_word_index = {v + 3: k for k, v in word_index.items()}\n    reverse_word_index[0] = \"<PAD>\"\n    reverse_word_index[1] = \"<START>\"\n    reverse_word_index[2] = \"<UNK>\"\n    reverse_word_index[3] = \"<UNUSED>\"\n\n    def decode_review(sequence):\n        return \" \".join([reverse_word_index.get(i, \"<UNK>\") for i in sequence])\n\n    X_train_text = [decode_review(review) for review in X_train]\n    X_test_text = [decode_review(review) for review in X_test]\n\n    vectorizer = CountVectorizer(max_features=10000, binary=True, stop_words=\"english\")\n    X_train_bow = vectorizer.fit_transform(X_train_text).toarray()\n    X_test_bow = vectorizer.transform(X_test_text).toarray()\n\n    return train_test_split(X_train_bow, y_train, test_size=0.2, random_state=42), X_test_bow, y_test, X_train_bow.shape[1]\n\n(X_train, X_val, y_train, y_val), X_test, y_test, input_dim = load_imdb_data()\n\n# Convert data to PyTorch tensors\ntrain_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\nval_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\ntest_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# Logistic Regression Model with Dropout\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegression, self).__init__()\n        self.dropout = nn.Dropout(0.5)\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        x = self.dropout(x)\n        return self.linear(x)\n\n# Evaluation Function\ndef evaluate_model(model, loader, criterion):\n    model.eval()\n    total_loss = 0\n    preds, labels = [], []\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            total_loss += loss.item()\n            _, batch_preds = torch.max(outputs, 1)\n            preds.extend(batch_preds.cpu().numpy())\n            labels.extend(y_batch.cpu().numpy())\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average=\"weighted\")\n    recall = recall_score(labels, preds, average=\"weighted\")\n    f1 = f1_score(labels, preds, average=\"weighted\")\n    return total_loss / len(loader), accuracy, precision, recall, f1\n\n# Save Metrics to Excel\ndef save_metrics_to_excel(metrics, filename, sheet_name):\n    if not os.path.exists(filename):\n        wb = Workbook()\n        wb.save(filename)\n    with pd.ExcelWriter(filename, mode=\"a\", engine=\"openpyxl\") as writer:\n        pd.DataFrame(metrics).to_excel(writer, sheet_name=sheet_name, index=False)\n\n# Hybrid FOX-TSA Implementation\ndef run_hybrid_fox_tsa(iterations=50, population_size=30):\n    output_dim = 2\n    model = LogisticRegression(input_dim, output_dim).to(device)\n\n    learning_rate_bounds = (0.0001, 0.01)\n    population = np.random.uniform(learning_rate_bounds[0], learning_rate_bounds[1], population_size)\n    best_lr = population[0]\n    best_loss = float(\"inf\")\n\n    criterion = nn.CrossEntropyLoss()\n    metrics = {\"Iteration\": [], \"Loss\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for iteration in range(iterations):\n        start_time = time.time()\n        for i in range(population_size):\n            r = np.random.rand()\n            if r < 0.5:\n                population[i] = best_lr * (1 + np.random.randn() * 0.1)\n            else:\n                population[i] = best_lr / (1 + np.random.randn() * 0.1)\n\n            population[i] = np.clip(population[i], learning_rate_bounds[0], learning_rate_bounds[1])\n            optimizer = optim.SGD(model.parameters(), lr=population[i])\n\n            model.train()\n            epoch_loss = 0\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                optimizer.zero_grad()\n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_lr = population[i]\n\n        eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = evaluate_model(model, val_loader, criterion)\n        elapsed_time = time.time() - start_time\n        metrics[\"Iteration\"].append(iteration + 1)\n        metrics[\"Loss\"].append(eval_loss)\n        metrics[\"Accuracy\"].append(eval_accuracy)\n        metrics[\"Precision\"].append(eval_precision)\n        metrics[\"Recall\"].append(eval_recall)\n        metrics[\"F1-Score\"].append(eval_f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Hybrid FOX-TSA Iteration {iteration + 1}/{iterations}: Loss = {eval_loss:.4f}, Accuracy = {eval_accuracy:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Adam Implementation\ndef run_adam(iterations=50):\n    output_dim = 2\n    model = LogisticRegression(input_dim, output_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics = {\"Iteration\": [], \"Loss\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for epoch in range(iterations):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        eval_loss, eval_accuracy, eval_precision, eval_recall, eval_f1 = evaluate_model(model, val_loader, criterion)\n        elapsed_time = time.time() - start_time\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Loss\"].append(eval_loss)\n        metrics[\"Accuracy\"].append(eval_accuracy)\n        metrics[\"Precision\"].append(eval_precision)\n        metrics[\"Recall\"].append(eval_recall)\n        metrics[\"F1-Score\"].append(eval_f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Adam Epoch {epoch + 1}/{iterations}: Loss = {eval_loss:.4f}, Accuracy = {eval_accuracy:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Run Experiments\nprint(\"\\nRunning Hybrid FOX-TSA...\")\nfox_tsa_metrics = run_hybrid_fox_tsa()\nsave_metrics_to_excel(fox_tsa_metrics, \"IMDB_Logistic_Hybrid_FOX_TSA_Metrics.xlsx\", \"Hybrid_FOX_TSA\")\n\nprint(\"\\nRunning Adam...\")\nadam_metrics = run_adam()\nsave_metrics_to_excel(adam_metrics, \"IMDB_Logistic_Adam_Metrics.xlsx\", \"Adam\")\n\n# Plot Results\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Loss\"], label=\"Hybrid FOX-TSA Loss\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Loss\"], label=\"Adam Loss\")\nplt.title(\"IMDB Logistic Regression - Loss Comparison\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Loss\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"IMDB_Logistic_Regression_Loss_Comparison.png\", dpi=300)\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Accuracy\"], label=\"Hybrid FOX-TSA Accuracy\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Accuracy\"], label=\"Adam Accuracy\")\nplt.title(\"IMDB Logistic Regression - Accuracy Comparison\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Accuracy\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"IMDB_Logistic_Regression_Accuracy_Comparison.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:05:17.280753Z","iopub.execute_input":"2024-12-03T21:05:17.281125Z","iopub.status.idle":"2024-12-03T21:21:06.810174Z","shell.execute_reply.started":"2024-12-03T21:05:17.281094Z","shell.execute_reply":"2024-12-03T21:21:06.80923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Foxtsage Optimization for Multi-Layer Perceptron on MNIST Dataset\n","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom openpyxl import Workbook\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load MNIST Dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\ntest_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n\n# Multi-Layer Neural Network with Dropout\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(MLP, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, output_dim),\n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten input\n        return self.network(x)\n\n# Evaluation Function\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            _, batch_preds = torch.max(outputs, 1)\n            preds.extend(batch_preds.cpu().numpy())\n            labels.extend(targets.cpu().numpy())\n    return preds, labels\n\n# Save Metrics to Excel\ndef save_metrics_to_excel(metrics, filename, sheet_name):\n    if not os.path.exists(filename):\n        wb = Workbook()\n        wb.save(filename)\n\n    with pd.ExcelWriter(filename, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n        pd.DataFrame(metrics).to_excel(writer, sheet_name=sheet_name, index=False)\n\n# Adam Optimizer Implementation\ndef run_adam(model, iterations=50):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n    for epoch in range(iterations):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(epoch_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Adam Epoch {epoch + 1}/{iterations}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Hybrid FOX-TSA Implementation\ndef run_hybrid_fox_tsa(model, iterations=50, population_size=30):\n    learning_rate_bounds = (0.0001, 0.01)\n    population = np.random.uniform(learning_rate_bounds[0], learning_rate_bounds[1], population_size)\n    best_lr = population[0]\n    best_loss = float(\"inf\")\n\n    criterion = nn.CrossEntropyLoss()\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for epoch in range(iterations):\n        start_time = time.time()\n        for i in range(population_size):\n            # Dynamic learning rate update\n            r = np.random.rand()\n            if r < 0.5:\n                population[i] = best_lr * (1 + np.random.randn() * 0.1)\n            else:\n                population[i] = best_lr / (1 + np.random.randn() * 0.1)\n\n            # Clip learning rate to bounds\n            population[i] = np.clip(population[i], learning_rate_bounds[0], learning_rate_bounds[1])\n            optimizer = optim.SGD(model.parameters(), lr=population[i], weight_decay=1e-4)\n\n            model.train()\n            epoch_loss = 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            # Update best learning rate if loss improves\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_lr = population[i]\n\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(best_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Hybrid FOX-TSA Epoch {epoch + 1}/{iterations}: Loss = {best_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Run Experiments\ninput_dim = 28 * 28\nhidden_dim = 256\noutput_dim = 10\niterations = 50  # Set iterations to 50\n\n# Run Hybrid FOX-TSA\nmlp_fox_tsa = MLP(input_dim, hidden_dim, output_dim).to(device)\nprint(\"\\nRunning Hybrid FOX-TSA...\")\nfox_tsa_metrics = run_hybrid_fox_tsa(mlp_fox_tsa, iterations)\nsave_metrics_to_excel(fox_tsa_metrics, \"Metrics_MLP.xlsx\", sheet_name=\"Hybrid_FOX_TSA_MLP2\")\n\n# Run Adam\nmlp_adam = MLP(input_dim, hidden_dim, output_dim).to(device)\nprint(\"\\nRunning Adam...\")\nadam_metrics = run_adam(mlp_adam, iterations)\nsave_metrics_to_excel(adam_metrics, \"Metrics_MLP.xlsx\", sheet_name=\"Adam_MLP2\")\n\n# Plot Results\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Training Loss\"], label=\"Hybrid FOX-TSA Training Loss\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Training Loss\"], label=\"Adam Training Loss\")\nplt.title(\"MLP Training Loss on MNIST Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Training Loss\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"MLP_Training_Loss_Comparison.png\", dpi=300)\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Validation Accuracy\"], label=\"Hybrid FOX-TSA Validation Accuracy\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Validation Accuracy\"], label=\"Adam Validation Accuracy\")\nplt.title(\"MLP Validation Accuracy on MNIST Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Validation Accuracy\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"MLP_Validation_Accuracy_Comparison.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T22:29:05.239746Z","iopub.execute_input":"2024-12-03T22:29:05.240104Z","iopub.status.idle":"2024-12-04T03:17:07.337117Z","shell.execute_reply.started":"2024-12-03T22:29:05.240071Z","shell.execute_reply":"2024-12-04T03:17:07.336184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Training Loss on CIFAR-10 Dataset","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom openpyxl import Workbook\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load CIFAR-10 Dataset\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n])\n\ntrain_data = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n\n# CNN Architecture: c64-c64-c128-1000\nclass CIFAR10_CNN(nn.Module):\n    def __init__(self):\n        super(CIFAR10_CNN, self).__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Flatten(),\n            nn.Linear(128 * 4 * 4, 1000),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1000, 10)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n# Evaluation Function\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            _, batch_preds = torch.max(outputs, 1)\n            preds.extend(batch_preds.cpu().numpy())\n            labels.extend(targets.cpu().numpy())\n    return preds, labels\n\n# Save Metrics to Excel\ndef save_metrics_to_excel(metrics, filename, sheet_name):\n    if not os.path.exists(filename):\n        wb = Workbook()\n        wb.save(filename)\n\n    with pd.ExcelWriter(filename, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n        pd.DataFrame(metrics).to_excel(writer, sheet_name=sheet_name, index=False)\n\n# Adam Optimizer Implementation\ndef run_adam(model, iterations=50):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n    for epoch in range(iterations):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(epoch_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Adam Epoch {epoch + 1}/{iterations}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Hybrid FOX-TSA Implementation\ndef run_hybrid_fox_tsa(model, iterations=50, population_size=30):\n    learning_rate_bounds = (0.0001, 0.01)\n    population = np.random.uniform(learning_rate_bounds[0], learning_rate_bounds[1], population_size)\n    best_lr = population[0]\n    best_loss = float(\"inf\")\n\n    criterion = nn.CrossEntropyLoss()\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for epoch in range(iterations):\n        start_time = time.time()\n        for i in range(population_size):\n            # Dynamic learning rate update\n            r = np.random.rand()\n            if r < 0.5:\n                population[i] = best_lr * (1 + np.random.randn() * 0.1)\n            else:\n                population[i] = best_lr / (1 + np.random.randn() * 0.1)\n\n            # Clip learning rate to bounds\n            population[i] = np.clip(population[i], learning_rate_bounds[0], learning_rate_bounds[1])\n            optimizer = optim.SGD(model.parameters(), lr=population[i], weight_decay=1e-4)\n\n            model.train()\n            epoch_loss = 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            # Update best learning rate if loss improves\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_lr = population[i]\n\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(best_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Hybrid FOX-TSA Epoch {epoch + 1}/{iterations}: Loss = {best_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Run Experiments\niterations = 50  # Updated to 50 iterations\n\n# Run Hybrid FOX-TSA\ncnn_fox_tsa = CIFAR10_CNN().to(device)\nprint(\"\\nRunning Hybrid FOX-TSA...\")\nfox_tsa_metrics = run_hybrid_fox_tsa(cnn_fox_tsa, iterations)\nsave_metrics_to_excel(fox_tsa_metrics, \"Metrics_CIFAR10.xlsx\", sheet_name=\"Hybrid_FOX_TSA_CNN\")\n\n# Run Adam\ncnn_adam = CIFAR10_CNN().to(device)\nprint(\"\\nRunning Adam...\")\nadam_metrics = run_adam(cnn_adam, iterations)\nsave_metrics_to_excel(adam_metrics, \"Metrics_CIFAR10.xlsx\", sheet_name=\"Adam_CNN\")\n\n# Plot Results\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Training Loss\"], label=\"Hybrid FOX-TSA Training Loss\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Training Loss\"], label=\"Adam Training Loss\")\nplt.title(\"CNN Training Loss on CIFAR-10 Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Training Loss\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"CNN_Training_Loss_Comparison_CIFAR10.png\", dpi=300)\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Validation Accuracy\"], label=\"Hybrid FOX-TSA Validation Accuracy\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Validation Accuracy\"], label=\"Adam Validation Accuracy\")\nplt.title(\"CNN Validation Accuracy on CIFAR-10 Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Validation Accuracy\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"CNN_Validation_Accuracy_Comparison_CIFAR10.png\", dpi=300)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T07:12:31.557458Z","iopub.execute_input":"2024-12-04T07:12:31.5578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN_MNIST_Validation ","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom openpyxl import Workbook\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load MNIST Dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\ntest_data = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_data, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=128, shuffle=False)\n\n# Convolutional Neural Network (CNN)\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10),\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n# Evaluation Function\ndef evaluate_model(model, loader):\n    model.eval()\n    preds, labels = [], []\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            _, batch_preds = torch.max(outputs, 1)\n            preds.extend(batch_preds.cpu().numpy())\n            labels.extend(targets.cpu().numpy())\n    return preds, labels\n\n# Save Metrics to Excel\ndef save_metrics_to_excel(metrics, filename, sheet_name):\n    if not os.path.exists(filename):\n        wb = Workbook()\n        wb.save(filename)\n\n    with pd.ExcelWriter(filename, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n        pd.DataFrame(metrics).to_excel(writer, sheet_name=sheet_name, index=False)\n\n# Adam Optimizer Implementation\ndef run_adam(model, iterations=50):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n    for epoch in range(iterations):\n        start_time = time.time()\n        model.train()\n        epoch_loss = 0\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(epoch_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Adam Epoch {epoch + 1}/{iterations}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Hybrid FOX-TSA Implementation\ndef run_hybrid_fox_tsa(model, iterations=50, population_size=30):\n    learning_rate_bounds = (0.0001, 0.01)\n    population = np.random.uniform(learning_rate_bounds[0], learning_rate_bounds[1], population_size)\n    best_lr = population[0]\n    best_loss = float(\"inf\")\n\n    criterion = nn.CrossEntropyLoss()\n    metrics = {\"Iteration\": [], \"Training Loss\": [], \"Validation Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1-Score\": [], \"Time (s)\": []}\n\n    for epoch in range(iterations):\n        start_time = time.time()\n        for i in range(population_size):\n            r = np.random.rand()\n            if r < 0.5:\n                population[i] = best_lr * (1 + np.random.randn() * 0.1)\n            else:\n                population[i] = best_lr / (1 + np.random.randn() * 0.1)\n\n            population[i] = np.clip(population[i], learning_rate_bounds[0], learning_rate_bounds[1])\n            optimizer = optim.SGD(model.parameters(), lr=population[i], weight_decay=1e-4)\n\n            model.train()\n            epoch_loss = 0\n            for images, labels in train_loader:\n                images, labels = images.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss.item()\n\n            if epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_lr = population[i]\n\n        elapsed_time = time.time() - start_time\n\n        preds, labels = evaluate_model(model, test_loader)\n        accuracy = accuracy_score(labels, preds)\n        precision = precision_score(labels, preds, average=\"weighted\")\n        recall = recall_score(labels, preds, average=\"weighted\")\n        f1 = f1_score(labels, preds, average=\"weighted\")\n\n        metrics[\"Iteration\"].append(epoch + 1)\n        metrics[\"Training Loss\"].append(best_loss / len(train_loader))\n        metrics[\"Validation Accuracy\"].append(accuracy)\n        metrics[\"Precision\"].append(precision)\n        metrics[\"Recall\"].append(recall)\n        metrics[\"F1-Score\"].append(f1)\n        metrics[\"Time (s)\"].append(elapsed_time)\n\n        print(f\"Hybrid FOX-TSA Epoch {epoch + 1}/{iterations}: Loss = {best_loss:.4f}, Accuracy = {accuracy:.4f}, F1-Score = {f1:.4f}, Time = {elapsed_time:.2f}s\")\n\n    return metrics\n\n# Run Experiments\niterations = 50  # Set to 50 iterations\n\n# Run Hybrid FOX-TSA\ncnn_fox_tsa = CNN().to(device)\nprint(\"\\nRunning Hybrid FOX-TSA...\")\nfox_tsa_metrics = run_hybrid_fox_tsa(cnn_fox_tsa, iterations)\nsave_metrics_to_excel(fox_tsa_metrics, \"Metrics_CNN.xlsx\", sheet_name=\"Hybrid_FOX_TSA_CNN\")\n\n# Run Adam\ncnn_adam = CNN().to(device)\nprint(\"\\nRunning Adam...\")\nadam_metrics = run_adam(cnn_adam, iterations)\nsave_metrics_to_excel(adam_metrics, \"Metrics_CNN.xlsx\", sheet_name=\"Adam_CNN\")\n\n# Plot Results\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Training Loss\"], label=\"Hybrid FOX-TSA Training Loss\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Training Loss\"], label=\"Adam Training Loss\")\nplt.title(\"CNN Training Loss on MNIST Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Training Loss\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"CNN_MNIST_Training_Loss_Comparison.png\", dpi=300)\nplt.show()\n\nplt.figure(figsize=(12, 6))\nplt.plot(fox_tsa_metrics[\"Iteration\"], fox_tsa_metrics[\"Validation Accuracy\"], label=\"Hybrid FOX-TSA Validation Accuracy\")\nplt.plot(adam_metrics[\"Iteration\"], adam_metrics[\"Validation Accuracy\"], label=\"Adam Validation Accuracy\")\nplt.title(\"CNN Validation Accuracy on MNIST Dataset\", fontsize=16)\nplt.xlabel(\"Iterations\", fontsize=14)\nplt.ylabel(\"Validation Accuracy\", fontsize=14)\nplt.legend()\nplt.grid()\nplt.savefig(\"CNN_MNIST_Validation_Accuracy_Comparison.png\", dpi=300)\nplt.show()\nhow()\r\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T15:43:15.540677Z","iopub.execute_input":"2024-12-04T15:43:15.541101Z"}},"outputs":[],"execution_count":null}]}